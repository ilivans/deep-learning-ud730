{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = text[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99959999 fluence the beliefs of present day anarchists others criticise m\n",
      "40000 anarchism originated as a term of abuse first used against early\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000 * 40\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z .  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 + 1 + 1 # [a-z] + ' ' + '.' (eos) + '?' (pad)\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  elif char == '?':  # <pad> symbol\n",
    "    return vocabulary_size - 1\n",
    "  elif char == '.':  # <eos> symbol\n",
    "    return vocabulary_size - 2\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid == vocabulary_size - 1:\n",
    "    return '?'\n",
    "  if dictid == vocabulary_size - 2:\n",
    "    return '.'\n",
    "  elif dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(27), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fluence the beliefs of pre.?????', 'pular especially in the su.?????', 'ers who play a unique part.?????', 'ryon asymmetry of the univ.?????', 'n to the throne in pamplon.?????', 'ndividual competition and .?????', ' with paul at rome during .?????', 'gdom declares martial law .?????', 'reece brussels daegu south.?????', 'les movement toward social.?????', ' the court to the other ha.?????', 'tter being of ted landsmar.?????', ' to gibeon to make sacrifi.?????', 'ng can escape an event hor.?????', 'ere arrested and constitut.?????', ' as a console game as well.?????', ' selected list of cult fil.?????', 'euds a blood feud or vende.?????', 'nd archimedes as one of hi.?????', 'e six zero s drug use and .?????', 'ns to add to labour cnd es.?????', 'to have been built in eith.?????', 'e from columbia university.?????', 'uction to hume s enquiries.?????', 'lso a change of au as in d.?????', 'blin transport space radar.?????', 'ining a digital library is.?????', 'on filter semilattice orde.?????', 'ecognised general english .?????', 'ion to rise along with its.?????', 'to the representatives of .?????', 'd astronomy which led to a.?????', 'ments to the multitude of .?????', 'church was no longer free .?????', 'dams american historian an.?????', 'h units flying from aircra.?????', 'eunions both of distant co.?????', 'utive mansion stationery a.?????', 'the colours in the palette.?????', 'cation main campus beaver .?????', ' president often has the p.?????', 'njab and other northern st.?????', 'ng realizing that she is t.?????', 'accept his daughter empres.?????', ' four seven one nine six f.?????', ' position serious mistakes.?????', 'r of his clergy either by .?????', 'frame in our minds the ide.?????', 'one nine one nine one five.?????', ' eight irish potato famine.?????', ' indian prime minister see.?????', 'companies and there are in.?????', 'her service which is alrea.?????', 'hatsky in one seven four z.?????', 'tivation or animal husband.?????', 'ear better than other leat.?????', 'ne three eight annabel tak.?????', 'ibal confederations some o.?????', ' as macedonia by non greek.?????', 'not publically fund midwif.?????', 't german aerospace agency .?????', 'llowing u s highway two is.?????', 's exoteric teachings into .?????', 'beled packets are forwarde.?????']\n",
      "['sent day anarchists others.?????', 'mmer when live performance.?????', ' in the history of the heb.?????', 'erse quantum field theory .?????', 'a references aragonese mon.?????', 'won gold in the team event.?????', 'his second imprisonment fr.?????', 'in ireland one nine one si.?????', ' korea tbilisi toulouse ra.?????', 'ism mas with two zero nine.?????', 'lf court games also double.?????', 'k an african american city.?????', 'ces as it was the most pro.?????', 'izon is formed and matter .?????', 'ion court was halted new e.?????', ' as a computer game today .?????', 'm actors michael berryman .?????', 'tta occurs when arbitratio.?????', 'story s greatest mathemati.?????', 'abuse increased greatly ar.?????', 't one nine seven nine and .?????', 'er one five six six or one.?????', ' s teachers college while .?????', ' a fascinating and sometim.?????', 'au r into as in d r this c.?????', ' image of dublin with the .?????', ' lower than that of a trad.?????', 'r theory general topology .?????', 'dictionary published in th.?????', ' demand which further comp.?????', 'the people but remained he.?????', ' b s degree in one nine on.?????', 'variables available in mos.?????', 'the patriarch was also one.?????', 'd novelist d one nine one .?????', 'ft carriers solved the lan.?????', 'usins and of disrupted fam.?????', 'nd that he had written the.?????', ' can be set as transparent.?????', 'falls pennsylvaniabranch c.?????', 'ower to fire ministers at .?????', 'ates in india also celebra.?????', 'he unintentional perpetrat.?????', 's matilda widow of henry v.?????', 'ive one nine seven one and.?????', ' in one nine eight four in.?????', 'representative or by encyc.?????', 'a of an infinite space or .?????', ' nikolay umov russian phys.?????', ' tipperary revolt in tippe.?????', ' also motilal nehru indira.?????', 'numerable types of eyelash.?????', 'dy free other forms of sca.?????', 'ero by the danish explorer.?????', 'ry with nearly two thirds .?????', 'her rather than wearing ou.?????', 'es a tour one nine three e.?????', 'f whom distrusted and reje.?????', 's international disputes a.?????', 'ery midwifery is not yet l.?????', 'in germany this project wa.?????', ' often called the high lin.?????', 'three general categories k.?????', 'd switched is the correct .?????']\n",
      "['anarchism originated a.?????????', 'ht wing election victo.?????????']\n",
      "['s a term of abuse first.????????', 'ry but in one nine thre.????????']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=31\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = []\n",
    "    num_unrollings = np.random.randint(self._num_unrollings - 10, self._num_unrollings)\n",
    "    for step in range(num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    eos = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    eos[:,-2] = 1.\n",
    "    batches.append(eos)\n",
    "    pad = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    pad[:,-1] = 1.\n",
    "    for step in range(num_unrollings, self._num_unrollings):\n",
    "      batches.append(pad.copy())\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def id2probs(idx):\n",
    "  probs = np.zeros(shape=vocabulary_size, dtype=np.float)\n",
    "  probs[idx] = 1.\n",
    "  return probs\n",
    "\n",
    "def chars2probs(characters):\n",
    "  ids = [char2id(c) for c in characters]\n",
    "  return map(id2probs, ids)\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 2, num_unrollings)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fluence the beliefs of.????????', 'bles available in most.????????']\n",
      "['ecneulf eht sfeileb fo.????????', 'selb elbaliava ni tsom.????????']\n"
     ]
    }
   ],
   "source": [
    "def string2labels(string):\n",
    "  body, tail = string.split('.')\n",
    "  body = ' '.join(map(lambda word: word[::-1], body.split(' ')))\n",
    "  return chars2probs('.'.join([body, tail]))\n",
    "  \n",
    "def reverse_words(batches):\n",
    "  strings = batches2string(np.array(batches))\n",
    "  labels = np.array(map(string2labels, strings))\n",
    "  return labels.transpose((1,0,2))\n",
    "\n",
    "bg = BatchGenerator(train_text, 2, 30)\n",
    "bs = bg.next()\n",
    "print(batches2string(bs))\n",
    "res = reverse_words(bs)\n",
    "print(batches2string(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Seq2seq LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix_decoder = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im_decoder = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib_decoder = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx_decoder = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm_decoder = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb_decoder = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx_decoder = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm_decoder = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb_decoder = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox_decoder = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om_decoder = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob_decoder = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output_decoder = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state_decoder = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w_decoder = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b_decoder = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell_decoder(i, o, state):\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix_decoder) + tf.matmul(o, im_decoder) + ib_decoder)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx_decoder) + tf.matmul(o, fm_decoder) + fb_decoder)\n",
    "    update = tf.matmul(i, cx_decoder) + tf.matmul(o, cm_decoder) + cb_decoder\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox_decoder) + tf.matmul(o, om_decoder) + ob_decoder)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_inputs = []\n",
    "  train_labels = []\n",
    "  train_weights = []\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_inputs.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_labels.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_weights.append(tf.placeholder(tf.float32, shape=1))\n",
    "  \n",
    "  # Unrolled LSTM loop.\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "\n",
    "  logits = tf.nn.xw_plus_b(output, w, b)\n",
    "    \n",
    "  output_decoder = output\n",
    "  state_decoder = state\n",
    "  inp = logits\n",
    "  output_logits = []\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    output_decoder, state_decoder = lstm_cell_decoder(inp, output_decoder, state_decoder)\n",
    "    inp = tf.nn.xw_plus_b(output_decoder, w_decoder, b_decoder)\n",
    "    output_logits.append(inp)\n",
    "    \n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output_decoder),\n",
    "                                saved_state.assign(state_decoder)]):\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(tf.concat(0, output_logits[0]), tf.concat(0, train_labels[0]))\\\n",
    "        * train_weights[0])\n",
    "    for logits, labels, weight in zip(output_logits[1:], train_labels[1:], train_weights[1:]):\n",
    "      loss += tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, labels)) * weight\n",
    "    \n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(1.0, global_step, 3000, 0.9, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = output_logits\n",
    "\n",
    "  # Sampling and validation eval.\n",
    "  sample_input = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    sample_input.append(tf.placeholder(tf.float32, shape=[2,vocabulary_size]))\n",
    "  saved_sample_output = tf.Variable(tf.zeros([2, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([2, num_nodes]))\n",
    "\n",
    "  sample_output = saved_sample_output\n",
    "  sample_state = saved_sample_state\n",
    "  for i in range(num_unrollings + 1):\n",
    "    sample_output, sample_state = lstm_cell(sample_input[i], sample_output, sample_state)\n",
    "\n",
    "  sample_logits = tf.nn.xw_plus_b(sample_output, w, b)\n",
    "    \n",
    "  sample_output_decoder = saved_sample_output\n",
    "  sample_state_decoder = sample_state\n",
    "  sample_inp = sample_logits\n",
    "  sample_output_logits = []\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    sample_output_decoder, sample_state_decoder = lstm_cell_decoder(\n",
    "        sample_inp, sample_output_decoder, sample_state_decoder)\n",
    "    sample_inp = tf.nn.xw_plus_b(sample_output_decoder, w_decoder, b_decoder)\n",
    "    sample_output_logits.append(sample_inp)\n",
    "\n",
    "  # Predictions.\n",
    "  valid_prediction = sample_output_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.375561 learning rate: 1.000000\n",
      "['ng class radicals including .???', ' policy and anarchist votes .???']\n",
      "['????????????????????????????????', '????????????????????????????????']\n",
      "Average loss at step 500: 2.867676 learning rate: 1.000000\n",
      "Average loss at step 1000: 2.764175 learning rate: 1.000000\n",
      "Average loss at step 1500: 2.636457 learning rate: 1.000000\n",
      "Average loss at step 2000: 2.576946 learning rate: 1.000000\n",
      "Average loss at step 2500: 2.508032 learning rate: 1.000000\n",
      "['the diggers of the english r.???', 'helped bring the popular fro.???']\n",
      "['eei ee e                   .????', 'eeie e e                   .????']\n",
      "Average loss at step 3000: 2.160964 learning rate: 0.900000\n",
      "Average loss at step 3500: 2.097931 learning rate: 0.900000\n",
      "Average loss at step 4000: 2.065554 learning rate: 0.900000\n",
      "Average loss at step 4500: 2.006832 learning rate: 0.900000\n",
      "Average loss at step 5000: 1.960332 learning rate: 0.900000\n",
      "['evolution and the sans culotte.?', 'nt back to power months later .?']\n",
      "['sniaaerea ena eni             .?', 'ea saaa sa                    .?']\n",
      "Average loss at step 5500: 1.952683 learning rate: 0.900000\n",
      "Average loss at step 6000: 1.922367 learning rate: 0.810000\n",
      "Average loss at step 6500: 1.895262 learning rate: 0.810000\n",
      "Average loss at step 7000: 1.854448 learning rate: 0.810000\n",
      "Average loss at step 7500: 1.836555 learning rate: 0.810000\n",
      "['s of the french revolution wh.??', 'the ruling class responded wi.??']\n",
      "['s oo eht trirreee           s.??', 'eht neiiia seii             ..??']\n",
      "Average loss at step 8000: 1.821808 learning rate: 0.810000\n",
      "Average loss at step 8500: 1.789261 learning rate: 0.810000\n",
      "Average loss at step 9000: 1.768461 learning rate: 0.729000\n",
      "Average loss at step 9500: 1.723728 learning rate: 0.729000\n",
      "Average loss at step 10000: 1.711015 learning rate: 0.729000\n",
      "['ilst the term is still us.??????', 'th an attempted coup and .??????']\n",
      "['ttih eht saat ea       t.???????', 'st na seattta           .???????']\n",
      "Average loss at step 10500: 1.690842 learning rate: 0.729000\n",
      "Average loss at step 11000: 1.699991 learning rate: 0.729000\n",
      "Average loss at step 11500: 1.670932 learning rate: 0.729000\n",
      "Average loss at step 12000: 1.669084 learning rate: 0.656100\n",
      "Average loss at step 12500: 1.625266 learning rate: 0.656100\n",
      "['ed in a pejorative way .????????', 'the spanish civil war o.????????']\n",
      "['de nnia eniiiie        .????????', 'eht sniitaa saaa   a  s.????????']\n",
      "Average loss at step 13000: 1.632524 learning rate: 0.656100\n",
      "Average loss at step 13500: 1.597241 learning rate: 0.656100\n",
      "Average loss at step 14000: 1.593656 learning rate: 0.656100\n",
      "Average loss at step 14500: 1.568437 learning rate: 0.656100\n",
      "Average loss at step 15000: 1.562188 learning rate: 0.590490\n",
      "['to describe any act tha.????????', 'ne nine three six three.????????']\n",
      "['ot eciteeoc ena eaa at.?????????', 'en enin eeehe ehe e  t.?????????']\n",
      "Average loss at step 15500: 1.545032 learning rate: 0.590490\n",
      "Average loss at step 16000: 1.535907 learning rate: 0.590490\n",
      "Average loss at step 16500: 1.530025 learning rate: 0.590490\n",
      "Average loss at step 17000: 1.496433 learning rate: 0.590490\n",
      "Average loss at step 17500: 1.512032 learning rate: 0.590490\n",
      "['t used violent means to .???????', ' nine was underway in re.???????']\n",
      "['t devo gnilaec dnaas eot.???????', ' gnin neh seeees see   .????????']\n",
      "Average loss at step 18000: 1.484171 learning rate: 0.531441\n",
      "Average loss at step 18500: 1.478922 learning rate: 0.531441\n",
      "Average loss at step 19000: 1.475496 learning rate: 0.531441\n",
      "Average loss at step 19500: 1.470937 learning rate: 0.531441\n",
      "Average loss at step 20000: 1.452271 learning rate: 0.531441\n",
      "['destroy the organization of.????', 'ponse to the army rebellion.????']\n",
      "['soreree eht noinerereroo e.?????', 'eeuoe ot eht elas so a a  .?????']\n",
      "Average loss at step 20500: 1.442564 learning rate: 0.531441\n",
      "Average loss at step 21000: 1.453020 learning rate: 0.478297\n",
      "Average loss at step 21500: 1.415311 learning rate: 0.478297\n",
      "Average loss at step 22000: 1.413530 learning rate: 0.478297\n",
      "Average loss at step 22500: 1.401542 learning rate: 0.478297\n",
      "[' society it has also be.????????', ' an anarchist inspired .????????']\n",
      "[' stsnoet ti sah lltl i.?????????', ' na nniiiiiis sehtoe  .?????????']\n",
      "Average loss at step 23000: 1.374417 learning rate: 0.478297\n",
      "Average loss at step 23500: 1.387563 learning rate: 0.478297\n",
      "Average loss at step 24000: 1.368018 learning rate: 0.430467\n",
      "Average loss at step 24500: 1.353436 learning rate: 0.430467\n",
      "Average loss at step 25000: 1.354925 learning rate: 0.430467\n",
      "['en taken up as a positive.??????', 'movement of peasants and .??????']\n",
      "['ne detaa fo eros oo     .???????', 'tnememef fo snneeead ro .???????']\n",
      "Average loss at step 25500: 1.342342 learning rate: 0.430467\n",
      "Average loss at step 26000: 1.368386 learning rate: 0.430467\n",
      "Average loss at step 26500: 1.351357 learning rate: 0.430467\n",
      "Average loss at step 27000: 1.315069 learning rate: 0.387420\n",
      "Average loss at step 27500: 1.313482 learning rate: 0.387420\n",
      "[' label by self defined a.???????', 'workers supported by arm.???????']\n",
      "[' lelen yb elas eeo e  a.????????', 'sreroow setuuuoow yb eh.????????']\n",
      "Average loss at step 28000: 1.300167 learning rate: 0.387420\n",
      "Average loss at step 28500: 1.323406 learning rate: 0.387420\n",
      "Average loss at step 29000: 1.317406 learning rate: 0.387420\n",
      "Average loss at step 29500: 1.298503 learning rate: 0.387420\n",
      "Average loss at step 30000: 1.321436 learning rate: 0.348678\n",
      "['narchists the word anarch.??????', 'ed militias took control .??????']\n",
      "['stcirirua eht yliw snoa .???????', 'de lanitnac seht serot  t.??????']\n",
      "Average loss at step 30500: 1.275839 learning rate: 0.348678\n",
      "Average loss at step 31000: 1.282559 learning rate: 0.348678\n",
      "Average loss at step 31500: 1.246365 learning rate: 0.348678\n",
      "Average loss at step 32000: 1.283120 learning rate: 0.348678\n",
      "Average loss at step 32500: 1.282398 learning rate: 0.348678\n",
      "['ism is derived from th.?????????', 'of the major city of b.?????????']\n",
      "['sii si seveses st e o.??????????', 'fo eht lorat oo e o  .??????????']\n",
      "Average loss at step 33000: 1.250717 learning rate: 0.313810\n",
      "Average loss at step 33500: 1.251568 learning rate: 0.313810\n",
      "Average loss at step 34000: 1.243038 learning rate: 0.313810\n",
      "Average loss at step 34500: 1.247507 learning rate: 0.313810\n",
      "Average loss at step 35000: 1.242135 learning rate: 0.313810\n",
      "['e greek without archons ruler.??', 'arcelona and of large areas o.??']\n",
      "['e reero tuortor rorrrrr trrot.??', 'ynororrn dna so srror r r r t.??']\n",
      "Average loss at step 35500: 1.239467 learning rate: 0.313810\n",
      "Average loss at step 36000: 1.228666 learning rate: 0.282429\n",
      "Average loss at step 36500: 1.225434 learning rate: 0.282429\n",
      "Average loss at step 37000: 1.212116 learning rate: 0.282429\n",
      "Average loss at step 37500: 1.209115 learning rate: 0.282429\n",
      "[' chief king anarchism as.???????', 'f rural spain where they.???????']\n",
      "[' suiro enil tsititaas a.????????', 'r raror siila hsoht ehc.????????']\n",
      "Average loss at step 38000: 1.201095 learning rate: 0.282429\n",
      "Average loss at step 38500: 1.212206 learning rate: 0.282429\n",
      "Average loss at step 39000: 1.204238 learning rate: 0.254186\n",
      "Average loss at step 39500: 1.199794 learning rate: 0.254186\n",
      "Average loss at step 40000: 1.202335 learning rate: 0.254186\n",
      "[' a political philosophy is th.??', ' collectivized the land but e.??']\n",
      "[' a lilililil etehtios si eth.???', ' davininonnnni eht enor eha t.??']\n",
      "Average loss at step 40500: 1.201710 learning rate: 0.254186\n",
      "Average loss at step 41000: 1.185251 learning rate: 0.254186\n",
      "Average loss at step 41500: 1.189525 learning rate: 0.254186\n",
      "Average loss at step 42000: 1.184840 learning rate: 0.228768\n",
      "Average loss at step 42500: 1.175023 learning rate: 0.228768\n",
      "['e belief that rulers are un.????', 'ven before the eventual fas.????']\n",
      "['e neileb mert ylrooc eht o.?????', 'nev emofef eha sleees s aa.?????']\n",
      "Average loss at step 43000: 1.184307 learning rate: 0.228768\n",
      "Average loss at step 43500: 1.188828 learning rate: 0.228768\n",
      "Average loss at step 44000: 1.198326 learning rate: 0.228768\n",
      "Average loss at step 44500: 1.152682 learning rate: 0.228768\n",
      "Average loss at step 45000: 1.179855 learning rate: 0.205891\n",
      "['necessary and should be.????????', 'cist victory in one nin.????????']\n",
      "['yreererre dna srrahs eg.????????', 'tsif stortof ni eht eig.????????']\n",
      "Average loss at step 45500: 1.152646 learning rate: 0.205891\n",
      "Average loss at step 46000: 1.161248 learning rate: 0.205891\n",
      "Average loss at step 46500: 1.137426 learning rate: 0.205891\n",
      "Average loss at step 47000: 1.163373 learning rate: 0.205891\n",
      "Average loss at step 47500: 1.126712 learning rate: 0.205891\n",
      "[' abolished although there ar.???', 'e three nine the anarchists .???']\n",
      "[' dagninina etuottoa hraht e.????', 'e eerht enin eht staaht a  .????']\n",
      "Average loss at step 48000: 1.129473 learning rate: 0.185302\n",
      "Average loss at step 48500: 1.141639 learning rate: 0.185302\n",
      "Average loss at step 49000: 1.135474 learning rate: 0.185302\n",
      "Average loss at step 49500: 1.149950 learning rate: 0.185302\n",
      "Average loss at step 50000: 1.141156 learning rate: 0.185302\n",
      "['e differing interpreta.?????????', 'were losing ground in .?????????']\n",
      "['e gniteeeee egiregeie..?????????', 'erew gnihaa sgnaaa ni .?????????']\n",
      "Average loss at step 50500: 1.115159 learning rate: 0.185302\n",
      "Average loss at step 51000: 1.125880 learning rate: 0.166772\n",
      "Average loss at step 51500: 1.117991 learning rate: 0.166772\n",
      "Average loss at step 52000: 1.135059 learning rate: 0.166772\n",
      "Average loss at step 52500: 1.136388 learning rate: 0.166772\n",
      "['tions of what this means a.?????', 'a bitter struggle with the.?????']\n",
      "['snnit fo seht seraat a a .??????', 'a retrir eratttow htih hh.??????']\n",
      "Average loss at step 53000: 1.113873 learning rate: 0.166772\n",
      "Average loss at step 53500: 1.124166 learning rate: 0.166772\n",
      "Average loss at step 54000: 1.126097 learning rate: 0.150095\n",
      "Average loss at step 54500: 1.117602 learning rate: 0.150095\n",
      "Average loss at step 55000: 1.092794 learning rate: 0.150095\n",
      "['narchism also refers to r.??????', ' stalinists the cnt leade.??????']\n",
      "['gtitecoc atra trerht ot .???????', ' sanitninii eht tna elae..??????']\n",
      "Average loss at step 55500: 1.122253 learning rate: 0.150095\n",
      "Average loss at step 56000: 1.090786 learning rate: 0.150095\n",
      "Average loss at step 56500: 1.098500 learning rate: 0.150095\n",
      "Average loss at step 57000: 1.107074 learning rate: 0.135085\n",
      "Average loss at step 57500: 1.102140 learning rate: 0.135085\n",
      "['elated social movements .???????', 'rship often appeared con.???????']\n",
      "['detale siitam srrettr hr.???????', 'citrr rehto serreroo tot.???????']\n",
      "Average loss at step 58000: 1.109976 learning rate: 0.135085\n",
      "Average loss at step 58500: 1.080819 learning rate: 0.135085\n",
      "Average loss at step 59000: 1.070744 learning rate: 0.135085\n",
      "Average loss at step 59500: 1.086172 learning rate: 0.135085\n",
      "Average loss at step 60000: 1.089185 learning rate: 0.121577\n",
      "['that advocate the eli.??????????', 'fused and divided wit.??????????']\n",
      "['taht ecaremaa eht eta.??????????', 'derof dna devises ehw.??????????']\n",
      "Average loss at step 60500: 1.076337 learning rate: 0.121577\n",
      "Average loss at step 61000: 1.075062 learning rate: 0.121577\n",
      "Average loss at step 61500: 1.074061 learning rate: 0.121577\n",
      "Average loss at step 62000: 1.076909 learning rate: 0.121577\n",
      "Average loss at step 62500: 1.071944 learning rate: 0.121577\n",
      "['mination of authoritarian.??????', 'h some members controvers.??????']\n",
      "['noitinnn fo ytatttttttrhc.??????', 'h emoh sreremm emerttorot.??????']\n",
      "Average loss at step 63000: 1.091136 learning rate: 0.109419\n",
      "Average loss at step 63500: 1.079663 learning rate: 0.109419\n",
      "Average loss at step 64000: 1.075757 learning rate: 0.109419\n",
      "Average loss at step 64500: 1.061651 learning rate: 0.109419\n",
      "Average loss at step 65000: 1.078473 learning rate: 0.109419\n",
      "[' institutions particu.??????????', 'ially entering the go.??????????']\n",
      "[' snonttntotit htitre..??????????', 'yalai gnitarea eht eg.??????????']\n",
      "Average loss at step 65500: 1.061564 learning rate: 0.109419\n",
      "Average loss at step 66000: 1.084477 learning rate: 0.098477\n",
      "Average loss at step 66500: 1.052762 learning rate: 0.098477\n",
      "Average loss at step 67000: 1.046260 learning rate: 0.098477\n",
      "Average loss at step 67500: 1.044247 learning rate: 0.098477\n",
      "['larly the state the word ana.???', 'vernment stalinist led troop.???']\n",
      "['yalab eht eraha erae eiiee ..???', 'tnereres stilatsaa roh eteh.????']\n",
      "Average loss at step 68000: 1.058778 learning rate: 0.098477\n",
      "Average loss at step 68500: 1.069381 learning rate: 0.098477\n",
      "Average loss at step 69000: 1.067743 learning rate: 0.088629\n",
      "Average loss at step 69500: 1.061742 learning rate: 0.088629\n",
      "Average loss at step 70000: 1.062310 learning rate: 0.088629\n",
      "['rchy as most anarchists .???????', 's suppressed the collect.???????']\n",
      "['hthr sa isoa trarat t eh.???????', 's seesporpos eht tsalpoc.???????']\n",
      "Average loss at step 70500: 1.069807 learning rate: 0.088629\n",
      "Average loss at step 71000: 1.025443 learning rate: 0.088629\n",
      "Average loss at step 71500: 1.050135 learning rate: 0.088629\n",
      "Average loss at step 72000: 1.056133 learning rate: 0.079766\n",
      "Average loss at step 72500: 1.043391 learning rate: 0.079766\n",
      "['use it does not imply chaos.????', 'ives and persecuted both di.????']\n",
      "['esu ti sela sas slalo     .?????', 'sati dna detarerres etah e.?????']\n",
      "Average loss at step 73000: 1.031665 learning rate: 0.079766\n",
      "Average loss at step 73500: 1.073080 learning rate: 0.079766\n",
      "Average loss at step 74000: 1.054896 learning rate: 0.079766\n",
      "Average loss at step 74500: 1.060483 learning rate: 0.079766\n",
      "Average loss at step 75000: 1.036101 learning rate: 0.071790\n",
      "[' nihilism or anomie but rathe.??', 'ssident marxists and anarchis.??']\n",
      "[' ssinonon fo ytrett eht errhc.??', 'tnerres stsataas dna etitthhc.??']\n",
      "Average loss at step 75500: 1.044898 learning rate: 0.071790\n",
      "Average loss at step 76000: 1.022608 learning rate: 0.071790\n",
      "Average loss at step 76500: 1.039339 learning rate: 0.071790\n",
      "Average loss at step 77000: 1.046776 learning rate: 0.071790\n",
      "Average loss at step 77500: 1.039050 learning rate: 0.071790\n",
      "['r a harmonious anti authorita.??', 'ts since the late one nine se.??']\n",
      "['m g duorororoa staa hcittot c.??', 'st etnib eht enif se eee e  c.??']\n",
      "Average loss at step 78000: 1.056201 learning rate: 0.064611\n",
      "Average loss at step 78500: 1.042356 learning rate: 0.064611\n",
      "Average loss at step 79000: 1.036709 learning rate: 0.064611\n",
      "Average loss at step 79500: 1.033472 learning rate: 0.064611\n",
      "Average loss at step 80000: 1.034509 learning rate: 0.064611\n",
      "['rian society in place o.????????', 'ven zero s anarchists h.????????']\n",
      "['nait yltorop ni eciht a.????????', 'neg erer s ssrarrraea h.????????']\n",
      "Average loss at step 80500: 1.015007 learning rate: 0.064611\n",
      "Average loss at step 81000: 1.035624 learning rate: 0.058150\n",
      "Average loss at step 81500: 1.039281 learning rate: 0.058150\n",
      "Average loss at step 82000: 1.016273 learning rate: 0.058150\n",
      "Average loss at step 82500: 1.040933 learning rate: 0.058150\n",
      "['f what are regarded as aut.?????', 'ave been involved in fight.?????']\n",
      "['g ttht era detaaraa na e d.?????', 'ema neem netramea en ehtit.?????']\n",
      "Average loss at step 83000: 1.046984 learning rate: 0.058150\n",
      "Average loss at step 83500: 1.028342 learning rate: 0.058150\n",
      "Average loss at step 84000: 1.033209 learning rate: 0.052335\n",
      "Average loss at step 84500: 1.015328 learning rate: 0.052335\n",
      "Average loss at step 85000: 1.028732 learning rate: 0.052335\n",
      "['horitarian political str.???????', 'ing the rise of neo fasc.???????']\n",
      "['naitarotat latiraraa hsb.???????', 'gii eht esis fo eht eeep.???????']\n",
      "Average loss at step 85500: 1.038375 learning rate: 0.052335\n",
      "Average loss at step 86000: 1.006019 learning rate: 0.052335\n",
      "Average loss at step 86500: 1.028289 learning rate: 0.052335\n",
      "Average loss at step 87000: 1.018936 learning rate: 0.047101\n",
      "Average loss at step 87500: 1.005986 learning rate: 0.047101\n",
      "['uctures and coercive econo.?????', 'ist groups in germany and .?????']\n",
      "['serurot dna enirreet erehp.?????', 'tsi stnara sa enanrag enar.?????']\n",
      "Average loss at step 88000: 1.036911 learning rate: 0.047101\n",
      "Average loss at step 88500: 1.024213 learning rate: 0.047101\n",
      "Average loss at step 89000: 1.010565 learning rate: 0.047101\n",
      "Average loss at step 89500: 1.017542 learning rate: 0.047101\n",
      "Average loss at step 90000: 0.998978 learning rate: 0.042391\n",
      "['mic institutions anarch.????????', 'the united kingdom some.????????']\n",
      "['cit snittatttiis eciira.????????', 'eht detini soronot eror.????????']\n",
      "Average loss at step 90500: 1.007562 learning rate: 0.042391\n",
      "Average loss at step 91000: 1.017524 learning rate: 0.042391\n",
      "Average loss at step 91500: 1.023654 learning rate: 0.042391\n",
      "Average loss at step 92000: 1.030033 learning rate: 0.042391\n",
      "Average loss at step 92500: 1.011538 learning rate: 0.042391\n",
      "['ists advocate social rela.??????', ' anarchists worked within.??????']\n",
      "['stsa ytararaa leira eerap.??????', ' stnorrawaw detrow sirahc.??????']\n",
      "Average loss at step 93000: 1.013526 learning rate: 0.038152\n",
      "Average loss at step 93500: 1.016043 learning rate: 0.038152\n",
      "Average loss at step 94000: 1.029064 learning rate: 0.038152\n",
      "Average loss at step 94500: 1.021441 learning rate: 0.038152\n",
      "Average loss at step 95000: 1.013207 learning rate: 0.038152\n",
      "['tions based upon volun.?????????', ' militant anti fascist.?????????']\n",
      "['suuis desaw nora sueed.?????????', ' tnamnnom iina ttsttep.?????????']\n",
      "Average loss at step 95500: 1.018885 learning rate: 0.038152\n",
      "Average loss at step 96000: 1.010171 learning rate: 0.034337\n",
      "Average loss at step 96500: 1.014051 learning rate: 0.034337\n",
      "Average loss at step 97000: 1.010611 learning rate: 0.034337\n",
      "Average loss at step 97500: 1.021608 learning rate: 0.034337\n",
      "['tary association of au.?????????', ' groups alongside memb.?????????']\n",
      "['yaat noitatttaaa fo ra.?????????', ' seorom egitanaaa eram.?????????']\n",
      "Average loss at step 98000: 1.015593 learning rate: 0.034337\n",
      "Average loss at step 98500: 0.992581 learning rate: 0.034337\n",
      "Average loss at step 99000: 0.998691 learning rate: 0.030903\n",
      "Average loss at step 99500: 1.004427 learning rate: 0.030903\n",
      "Average loss at step 100000: 0.994397 learning rate: 0.030903\n",
      "['tonomous individuals m.?????????', 'ers of the marxist lef.?????????']\n",
      "['suonooot slalnannnna a.?????????', 'sre fo eht esaemem e s.?????????']\n"
     ]
    }
   ],
   "source": [
    "num_steps = 100001\n",
    "summary_frequency = 500\n",
    "mean_losses = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    labels = reverse_words(batches)\n",
    "    batches = batches[::-1]\n",
    "    weights = np.ones(len(batches))\n",
    "    weights = weights / weights.sum()\n",
    "    \n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_inputs[i]] = batches[i]\n",
    "      feed_dict[train_labels[i]] = labels[i]\n",
    "      feed_dict[train_weights[i]] = [weights[i]]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      mean_losses.append(mean_loss)\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "    \n",
    "      if step % (summary_frequency * 5) == 0:\n",
    "        val_batches = valid_batches.next()\n",
    "        print(batches2string(val_batches))\n",
    "        val_batches = val_batches[::-1]\n",
    "        feed_valid = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "          feed_valid[sample_input[i]] = val_batches[i]\n",
    "        val_prediction = session.run([valid_prediction], feed_dict=feed_valid)\n",
    "        val_prediction = val_prediction[0]\n",
    "        print(batches2string(np.array(val_prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmIAAAAiCAIAAABz8yL0AAAABmJLR0QA/wD/AP+gvaeTAAAACXBI\nWXMAAAsTAAALEwEAmpwYAAAAB3RJTUUH4AoHCQ0Z0+N8OAAAHytJREFUeNrtfX14U1We//cE8N4i\nmuALuXXR3Dpob31LWGdMcEYbGZ9pXF8adNeGndVGZWgwYgKoDbMwxIGxwZ+asPxqI6KNMrONM4ON\no2yj45qoszb6zG6izJggai7K0oAwSVXIDULP/pE0TdPk5qWlYL2fhwdKk88953zP9+V8zzn3HIQx\nBgECBAgQIEBAIYgEEQgQIECAAAFlhsmIhUYIIYRUrpggnCy4kJmuTiQxlwpRhhCX8yy/VkJqvFwl\nxZegcF4NSWr9XHWNi1iYdJ8jxsEKfS1gXJYSceoYEiGEkMJZpTZxfp0EqT2J746XKNeTpDHan0xS\np3xHELFlnOFoQRXIJuVdUYwDeipP1AEDJSngimMuFaLMIW5KC4+UqLStWoackIdRGp1eS59oSvlg\nbBGMcbij/iQJNxFwGTQMRSKESFpt9mTdDMd6bToVLUEIIUqhc44oWVFKzO/QqxkJiRCSMFqbPzHi\nKdwWrYImEUKIUhncEa56ShUVzo5mKENgQkwl5lKRjC1yqhlKwms2eyhHcCAejwcMZWgsF9BTSOVO\nfCe9REXDW0ofwBgPdCtPbKdMIaMrSuGxIMYSwclwu4w/mxRQGLTO4bKoJBNiTIzB6TRUFHKroHyb\n8g+3O6GwuPzBYJ9NEdmk07ljWTX3S/QOTzDY36OFF5ZpLRkTKErhIi4vaKyeQLC/10B5V2sNwwbI\net0R2uz0B4O+Lk3sqcW64cFi5ZQqKvydQYJlU7RGraAkEgkpeIlvYadMHaMrTqkCeBTC7bJ0NjkG\nyf42qbjZlxz5TdQuz3+Ysnsg82G8394iFwMAiOub2vsyv076WsSg7LC3KqUEAFHf0h3OPjAZ7m5V\nSgEACFmjqSeaTP+yXUbImhplBEibOuyt9QQQ9W198TQj2mtqlBEAAFJla3cw86hkMLcRyf5WMTT2\nxnE+hZDKm+3hJOZHzshipHEYYxztkoOs1W5qqhcDgLTR5IvntkQuBgCxvKWjXQ7StnTVkv2t4vSj\niKa+kYLjfc1iork3nitpUKbrX4SCMcbRXlOjFACI+uaOjkaCSPdN0teS29y+RhC3ZDot2tPWJJem\n216flXC25zvqod6e1/MViivZ3yoVy5uUUiDqW+wdzTIAQtneX0rGox8A9R3hsZ8MdCsB8kXAS4n3\nNRF5fZbTVGlrgWpVTqmowsm+JkLaVoE4eDDQrSTGlJuM9pga69MGIW/pCsZH2Wq9qbe7LWNhclN/\ncsToCAAAsUzZ2h0t3UU8RldgVornQb4WIo+RkXDaS3R1D+t3rpfAA33tzfUZx9LcMWJ0FStkMtzV\n2lifaXt9c9ZHpVtjkoHc3pv1YY32MMbR7iaxuGlYRvG+Fimh7Agn+bxEMQnHexvzfWdBLSqoVsP+\nJFdz++0t8qzz7B2otlOmntEVpRS0oBGfP1pQ5YfJYLtSXsDGB7qVIDXl9dtAT5MYZK3d/dFosNck\nB1CmHXDS1yIGkLX2RpM4Ge5qJIjGnrR84r5WKUibu3zhaLivo5EAmSmYzNRY2tob9JlkAPUmX7iv\nTUY09sQxTgbb64Fo7OgLR8O+rmYpSFvTRsMTJgd6GgmiscMXHhiIBn3d9u5weT4rGTTJCoRJIJQd\n/XGM4z6TLFPhzHfFTXZfOBrsbZMD5Kl10tcsHh3z0nGyLz42Shal4LBdDoSyvS8cDfe1KwmAkmEy\nGbS3tXf39Yej0bCvq0WarXDxMFmpuJL9rWKoN/mCva1SAKW9P9jTLM6G6TKkbG8kCKV9rOoO9LbK\nQNbaFy+bEveZ5CBu7hkY6+a7m8VQ3z7G01RBqbDCSV+rXNkRnJAwGe9tljeNjmrxvhYpIW/r9oWj\n0WCvSUmIR8ZeUbscQCxr6vBFk8nkQH9vX3RYi8TNXf3RgYFosK/b3hMt2UVFjC5bTn2Z/j7Xfyl7\n8uTUIgaQNnb0BsPBXlM9ZL1Est8kI2Qtdl8wGg33dTSJiUbeyM6nkHFfe1tHjy8YjkbDffYmMchH\ntD8ZNMkAxPUtXf0DyWQ82t/rG0h3a4uUaOyKYjzQ2yxO/8TrJfglnOxrGh7eVhK58sNkMtwhJ6RN\nHb3BjGkT8lFdUHGnTCWjK0opYEETECZ5+y0vTA50KyHP79e3h4cNIOuM4z1KyIyu433NuUOK7Dgi\nGW6XEY29cZz0taT1O97bSNR3hDNPzXb+QLcSiKa+OH+YDHfU50UHPJ4wmVXxZNAkzXw+etyS9LWI\nS4ZJHO9rFosz8kr2t0mJPFsbSwl31EM2MUkGTbLSYXLMSI0Y1dGFwmSl4kr2t0qlw50kMwWTONol\nJwqOFAuIuENJEMr2sflBWvtb8rJfPkrcZ6onxE32sTWPdjeJifq2vgE8bkoVFT6hiHbJCdmIV0n2\nt0qJpmFFiNrlYxOdtK5WYu7FjW7Cw6SsPZwbGNJeoreRkLaOqHO4PU+Jq1bIZH+rlMiaTcaimnrj\nBXKfVhkhb2tvEoubxohzrJfgl/DEhMmkr0WcW9eBbmWuIlQXJqeO0RWn8M4gju60E7E2ycVCMaBV\ntGR4ZU1NAxtih5dpqOwHEhK4BAcAkIiwKamCGZ7Wp1UMsT8SSc9XkyRJpv8e/peLcRzHRvYTjGp4\nQVqiUEhTbKjE5DOtNTTGNs2nVRq92ebys+NcORppCUkONyUxql4krS5jH4NEZVCD3xlIAHAhlyeh\nMGioEhIOsECrM6uV5RUCiZBTr0ovjiPUsHpXKiP7CRaXhAQgSQCSIkkgSRI4rgwe6zKsjmg9Xpta\nkr/Hw2J4TmL3unU0WSYlYjNs4gxej1mRx0h4DAa/wuV3jhFv5ZQqKnxil3hZbyS1Z+P8msx2yJoF\nz+1PxWI5e2NkKkVeVUmFQS9/fxlDq7V6s8MdiJUupAqjq3KdT5URuIQaNi2O9UdS+5+7briJqGHj\nrlQiUkqLiylkzG/TKShyRFp59sBoFGOXGSUah0ufeGpjQO106anSuwoqlXA1u7lCocHUq4tmD0ul\n9q53Uwl2XJuippDRFaVUhO/WFh6SMfsT0X6nXk1GXHddx2hc7Akphiz0I3+c1IDfGUiUFyXLLiXH\n7BN+g3qZn7EFBpK43I2tkyQuAOASIRZoNSMp6JnFjIIun8LFIjljiPy9DJSqUCEVU6qo8CSgMS//\nCeXuaySpMWpCqmyheNhr0zEQcixewOhO/bcw5HkzHqxDUdVoJObWaVZH1M5QPDmcJo/ZuEoWHqEG\n9gAMhvyRcmQ1SRLOn/NJuNXjGaNNHaMrTpnkMEkCwOh8gaQUFLCB4QENF/GzQCt4x9YShib2h7KK\nxwYiKTHD2zaSZqSpSIDNpkqh/QTNUABA5mSpAFyMHcwjqrQGi9Mb6GuGN12hidZaCa2QDrLZtrPl\nDSAlaoMG/E6v3+VJqEpHSZJS0RDLPjoWYkcZdzZ/yxlUxryhQbnZqlNQJAAk2ACbH3RJ4GJcATlP\nqLgSMZaNJQoFZLfPqaUKScbh81gLuMKiFFJh9fochQaPlNblcxfaMVw5pYoK80olFouNb2qDpNUM\nhNyVd5CEUesMNpc/0K0c9Lr5t+YWN7qJ9B/FS1cxRMQ7ISkZx3pDqUaLVctIyMwcUHlpllVrTbT1\n+TokLp3ZmxivhMly2z7KTEdTKIVCvMfvj02cNU4hoytOmdwwKaEY8X6/OxDjRnqP0pib4CWD2R1g\n2ZDHYnhqv9ygZ/jTKbNO+q7F4PRH2IjfZrDukukNvK6GVBh0sl1Wg80bYdmA02B5V6w1p2VOqxTi\nXW5PiANIBJzWN3PSfJfZ4vKH2FiMDXhcgZRMMeETY6TCoJOFbDZ/AoBj3Vb3/vJoKoMGXjKbXQmV\nvoxcktEZlKzL5okBQMxrc+waUQuNggg5vWzmg/ezqqeiiYgnPXEa81vNL6XyVFOhkO7xOL2RWCw2\nPMSYeHFFbOq6OoV5zLuDXMRjszgKmXos4LRYXZGxvqQohYu4rFZnoID1x/wOi83DchNAqaLCPL7X\nrKitVVvH9xYkrbO2wAs6nc0bYtlIyO+26fUlsn8u5DDb3P4IG4tF/G53BGgVfwfzGF215kIpKIi4\nvZFEGXPzEq1NT72q11k8AZZlQ363w6C3VffODUmpGIh4/DEA4FiPxfJmOVHFb9ZtBIPboVFb3Dba\nrdOXesWghIRphkqF3F42wZUbLEmFSjbodXhCI2ZKqqwW+fsWrcEViLBsKOBxmnXmct7O5QJmpq5O\n7YxMYaMrTpncMEmqrc5W0nVdbU1NTc3wERSUzu23q0KGBXV183Veqr3Pa2ZKpVNOb7cmZr2uoa5B\n4wR9j9emIksojM3fowentqGuboGV1XT5XRpJpkoOVyvY5tdIaLUtoW8W54zFuIBNp6qrra1TW1lN\nt9dWYtSfcKsRQqhm/qY98O5dtekzMPhVkFTYvE6VX0eRElrjprXZyU3WoUAIoZrrXhpMvXpDDUII\n5bzeSqoMGvH+XalRuWRxCm32uHWsgSYllMpGakdezZFoHE5tzFCHJLTWzeiVIy7G5dZzFkYioSi1\nDQym+vx+tLlMEvcNDbW1tSpHpDpxjSPLDAXeDRVa/ORioXcDBXOkYhQuFnrzzVCB5IxLRALvBgrN\nlVVBqaLCPFPOCQBqvBNDlNYV6tGBSze/rq5BpbN6QVHikSRJxjxWraKutrZB4+D0vZ5SL58XN7rq\no7uzQxUxNMyuqamh9CW8O6lyBvqsEq9hQV1d3XytxZ1g0rMjVZSrd3VrInqalFCM1kWZW6QlVdRj\n0D0laXenHRNjdjsUXr3OxfJ5CX4Jk4zZaWK8i+pm19TUlHdYBMlY3B2KwOL5tbW1TMYTkYzF329j\nQhZ1Q13dfI3ZxVIqalyGOoWMriilIqDRR59HLHSD1xINlXOAhgABAiZg/i9kZua7Nf2sU0UK0hAg\n4GQbZMTC5EXBMdkkCe8vq0OkWjjTVYCAyUDM708orRYhRgoQcLIRsSlQTcPGPfzZpAABAgQIECBg\nBMKZrgIECBAgQEBRTBdEIGBy8DR+8rvZ8J+he4UezBXCidCEKSNkwR5PQXusMkw+g50TWO97kOG7\noEmCJQsQIEDAtz2bjFjo9PqlsnsgoKdOcuV4ohRPyKkitlURwMzvGY+oAABIFzju3CwqNX2dAk4v\nQc8NAgAoe+IB3Slx3w6/rIS4LqAgDqGDDz+zjlsCAHDb0A1NcNMEPjwJR1YufPB63zW3Yd13MI0T\njO5kImJjGlbvAgB5V+5O1wLZ5OgvDIMLGGhNzB3zDJ+CNARD30Y5HECxtX+3/q7/XazCPxrPcxxX\ndcIQmN8zDpX3VjgBpCuBXVzAQC8ICeqYg50ouLVna/I+gL8B8Qi0r35oLpYBwDdwdBt69s837zz2\nCoguge//9dK7sSE9HClGOYQO/mpg3ZGFMBSB6VrQvXjbtbAwXcpr8B9/eGDH0ScAAE7/CDbM23g6\nzKqOUkWF0ybk1Uj0tH9CXvyIuVS0TRuKWJgTM/PB769fgG3f/BoseMXZcPYZWDyZ2jIEQ/f/Ynn9\neno5fvBExKrveJSaSkZXjFLMggDS1zKbIxamwTtKLMIWHgEnGX54vWbx9LZDrfqhlmNvwcb1jx6H\nYwAwiBLv/25n8x9uWIWNp38A703763voHX4KC5/iA3Dnhy3L8F3TtfCbedsPoFjat/Z9sOOHj1+1\nAi/7MV5w+O/BhZ5KP6oKShUVnmLYvfvjGbcCjS8U49kiwYdMIUwloytGqQLjUvFv4Oiv4Bdt5xmX\nioyGK4xPIns6xdyNIkvPMe5GmTxr1THj8l8a01U8hA6a/mBcShqXiozm14xp6QAA61QgWu8waxgJ\nQohSm/2JnBmeghTgWI9Znb7yglLpXTmnVu1EwfssxqUiY9tc40NoebrofWjv0nOMa9B6vA+eRT1L\nRcalpJFFn5ScX8qWfu+txnfgrZJieR49fe8txqWiMRUuAi5koRFjdhjUtAQhRGsc2abEvBatipYg\nhBBJ57SxKIXz6yRIZXPoVRSJEMnosic4FZNwEo4sW2j8BTy0/FHjUpHx3luMO1GQv3S+LKdyynL8\noA1vuhJfdTVcO/MPcPx/4BgcA4Bz8JxN/9jZBDfV40vap61Fc+AL2M9PuRJfZb+i82q4dj7+vu7O\nFrwPjsBhABCByH5F52Lc2oAv+yf8z6eZYffbn6Z1tQpKFRWetLz8vpUZxVtuM+5Fe7IZ2PJ/M67c\na1x11Lh0pnGpyLjygDHdlgD6k3FJxlLsyFbSj5hfNy4VGQ9fDNz9YBAtXyoyvgqv8BgdP/ahvfc/\nP2Iph9DB7Ef7YeD+Z0cpZAo4o8FoEC0/ugF2IjbNehltL1VIIuDQKai0RqrNnlhJcfHoMMe6zWom\n7XIUOmcoAQBcxEKTtEZNk4jS2Bx6hkQkY8ie9xrzWrSMBCGEJIzWljG6J5F92ULjvbcaDX9v3IL+\nf9sFxntvLcNRFCi9dOO1CglCCCEJrTZ7S70NP5WMrhjlhIZJUkLTeUcg+eD1z6hDi/ct2oDX3rPz\nDinUlpwweXjbumO/h+WpJUZ89zduWL9p/Yg57XG7KWsggeM+HbtJbw1xvBQuZNUscoLBE46GfVbK\ne5fGkNbBg+jAkwu2Tv8HWIMfaNnbnDh76Cm0GQDOw3O3HOzcgNei8+BuvHjLUOcWrpPG3+Ov868G\n1h37PaxKGa14dfOLN8xAp5Vs49fw9Y0vX78GP7AKG4c+Gt3Gotjl8quckQQe6FUFVuidbMYwYhyj\nd3rD0WjUb1ME7lLrR4YPRSgA8K4zpHWzXDLsoDwGQ45jKCThNL5YeHhF+4rNQ4/PWAJb1m9NV5i3\n9CKWXDllJEdBkcNXwGW98wgg80S68av1IgaU8MMyKYMo3vPLF8gtcD7Ov799B+o9+gT89Jo78tKg\nKiiVVpikGHqCDhEmJRQz2hxTwDnbtk6/FdbgByx4xbWr82V1WAXnEX9nP7LxEbzu1jn/JALRThTs\nPr/n0mfmrcEPtO5t+WjB579B3SXWGq7v3DLUefpHQP4bOIc2bxnqTC9MFjM63sQlvuHmjmnfhwfx\n8lXYePwv8PC2dVlL+eDyj5vuuX4FXiaal1FIAshOZ6dzaPNpa+ByTG8Z6twy1Hkzvo1fHyM2tdqW\n0DmD0WjYa6HcOk36UDgecRXV4YRXr9L7GYs3HI0GnWrWrNZnLvxIcZTZ4zWRr652SpyhPj3nsnkT\nAMAFzCqtmzR4gtFo2K3nbBrt8Dm7OAE/671r2lUQWhRe+bkRSeA/4VXeiFe0dJ4BuFW3IqRwBqMD\n0bDXoaMrmtyeGkZXjDLWgnhQ9k5XUmEL5K+pxdD/ii6D+ehKMZ49B6gfgIr/GZ+hKGeGfzl42+V4\nPgD89NnDrpkvfG4aPvKA0NjMKgkAqPRa6aZAKHGpvDiFCzldu+qtHouGAWAMTpur1mALONQaST+8\nPfQp/Lxx7RxMXQB1bxx87S+LPk69yOWJtUxdOfY6TL8d5uGLp8H082Au4JLjDtG9eEX2v7r7Wp5n\nXvjS9OVsfBYvT6q36hgSgNIY1ITGG+HMNAlA6x22zBdo2mpxuKwellNnTlYtTAGAerNVS5MAjM6g\nWGb2s5wuow6FJJzGJT76QjwPAG685cYXf7nj67VfifFs3tILowpKVpUf+97mWR9CG16e1wUPfr38\nyC2wymeag6lyKIMo/q8r1iAa1t+xYRoepeEvo+2vSN9oPnz9D7BqnJTKK0yqXaHABCWOEq0npM3X\n1ePvwI+euuoCXAcAF8K8PMqM28GMH5oG00/Hs34EcwDgmT9urfk1GLBJBKILoC7U/9/v3hNavPXo\nDDit0vpUYXSvw6v4KFgv3SDGswFgyaq7n6x79tAdB8/AZwLA6e/AjXgRADTZF+648o20QlYsJi5g\ntUXULr9FKwEA2uCwuWirK2S2KXjEVUyHWbfFQ5oDDr2CBADa5tC51U5/QsMAEIxWrVBJVGIXaFWM\nIkGDn40BgNfi5HRel1lNAgBtcRhcCof39tajADDjVrgCK4gNcDwAF2Fmxj/DJ3g3T1OKlK7V8u0F\njEViBK3RKGgJAEUzqvL93lQxuqKUsRY0EWGyEK6Fhe/sC7Vfuqbmebj6yh/ehLU1MJPn+1/AF3AE\naFSXDjY0uhBmwmd4DyAAKHzLcVFKoRti/aEYaMiPYfe0q0GMJdlKbn9nx9foawJXHCZFILrlX27+\n7cyX79eZiA1w03nNari+5GLMW/DGb1duP+oY/v/McrY7Uczw9XakhEyxHJe+ZSfiNhus7jd3DV8G\nJmW4EhQofPF1UQmncR6cn/5hFszCCRgCDCVKLzZ6r5iS1vLN2zaTDnj0tE3TRuvkThT86gJYHl8y\nD19cJuU34Dr+AdjsG/Ic6yCK77j8jcb930/73/FQqqjwiUYNzDw9AD703juPvCe21LSgn6ZHllmc\nthJQjuqmgDv6LBxzgwFG3Me0hXAEHRbjisNkFUb3p91vH38NHkRrcqoIg5A4A84EgKvPuCZt8mfj\nc7MKWTFiodBgatei2Sjnd2I6wS+uIjqcvvj6/fk1G3MeJo8lgMm9Oh5GXR3vj6T2v3ldzXM5DFnk\nCDoXANA5gECEJIAkAABIAn/bGx+aO1R0wrVI6cATJkmVWUfdsJhmnBq1Sq3R6bSKsjbYTxmj46FM\n3tokjb+3+UP77X9pFl0Mf5z9Xw/98sFv4OgUW9O+HjSPJNdd39049CG4RS91gJU/5u1De399zvbz\nn5BuxOu3DHX+DN+Bzqy2bC5k0Sz2SMze9FXKAz3KSWx4CriKS6+2wt/A0WMeqL953lhV3o/3o7Ng\nLsjKp+ze/fH0a2EWPiN/1hEO4wNwETQUyGUrpFRR4UnA4zM7H8TLf7BaMbghuXna1rfgjdxPz5pb\nYLuNHF+Ynr1M/+l6vbOapK1aEFboGto0UgGu8yLMTHQhRa8sLiwufh3mvfi6IMbeI129hCsuXaJx\nsQNBt0VLJfyWRfMVhkBZd2VMGaPjoUxemASAGXDaQvjJ47M69X9rST0KcfQ3AJgBM9D0zKosABz3\nZ758LpwLM4HF0cw0Av4UjsAFiM+hFKUUvSGWnAcXHX8HBlEim9tNuxpm4VnZBBEAjuIKwvk5eM4i\nfLvj+s45QzX77j6Uu9CIzgL4EnBO4PwMWHQaLEX3pWdZw7ATfzlqgDf2XtXiqxGhwB6ZzqpPX4vD\nsX52En3uIEpUXHoZFY6hfYMonu8ugbxne+ut0DL2kVeiq5Z8cseZ+MzyKcaLTPeuWzbWMM7Fc5bs\nv+MyfMX4KVVUmFdu47+WOYOLMHMn/tmmNZtP+zl49m7nGdIRQM64EyKbPq16+18u+I2uIFQXLTj6\nPORu26lAOfFgWd8rdWVxAXEV1eHKL76eyHukq752GyiFRm92uAOBjvo9XveoOxkPogMF9w1NGaPj\noUxemNyBenvQc5+h6D6093cfvzBNBQQQACDFtaJL4GXcOwRDO1HwyPBbwhfgOvJReOHO7TtRcBf6\n8Dd3v0BsKLB4m4uilOI3xC6Aa9AceOTt9Z+h6Bvw2oFzkg29dHaN5HQ8S8SAZ9/2JBwpmfsOwdCT\nyP4GvHYAxXahD+P3J6ffCrn9+qOLrkk9Af+D/nwQHUi7m7PhHMxBEP8ZAFj0Sf8PQ2O0nXjf6fKz\nOdcfFx0L0ox0j98b4QAgEXBYXJO5b/IMfGbFpZeqMIs++cXsX617e02e+04Bt+3V594cnf2k8Vf8\nwbbN275GX41NdotRXoYXn/68a6z3P4QOPv/4to9QePyUKirMk4NPyLXMgyj+C3jov9F7B1DsHXjr\nm3+HH5x/Ff8CwZIblqTWwEPHTJ+h6G4UeR49nd2sXin4ja4gfgw/EV0A6zet34mCn6KPd6DelZ8Z\nS5qkCETT/wFiK+MH0YHSc1fFrywuKq7iOlz5xdeF75GuTsJVXLsNCb/N7PAEIrFYLOJ1eViCybmI\nMgXc2h8/bF20PgXcVDU6HsrkhUkC1/zpjvc2oMesqOMbJ9z7n0vS8wk1MHPxG7ftvSy+bO7yrT1b\nZ74youLr7nl4+j/CZrT1cdQ5/SewdtVa/nS4OKXoDbHn4DnGnUuOvQIb0GPus16SHBq1p6YGZi56\n/cYjPwGT6EEjuYL/hZC0l/nd1S+tQesfR50zboJ/vXltruu5CWtn/h6eRtt+jh7+D/RSenx6Xfyq\n317yUttc42Ornrjyvy4dPekq0TpdrZzturra2lpNicvPSbXD3U45VRKSolQWVmedzEnXGphZcenV\nVngIhlI22I0LBIov0IHUBjgGx8un7Pnj5+nXk/PwFXyZ2lA4EamUUkWF+eLkRFzLTOKaLwOHt3zv\nuTVo/ba5v2M+Of92/FN+yuV4/t1fLU49BBvQY/8PbX5vVehi3FDde5D8RleMsta3WnQebCa22pB9\nx6LXGVlZpZsXrMQc/Bw9bBStKPVCSNEri4uKi0eHq7j4utA90lW+aVp56UBKgHWZNQ21tbUNOg9l\n9bh0ZZ2sNmWMjodSEaq8lvkZ7JzAU3iKnXxx0g+rm/Jnuk7muVnCUctFguTEXMs8CafwTKWjz0/i\nKTynyGF1gj0WM0jhWmYBAk4xCNcyCxBwiqDItcx5E56MLYJt5T3wlD2kSjg7WMC3CbQ5lDALYhCy\nGQEnH4wlhC1jf5036XpqobpJVwETImFByAJOitYJsUfAqYb/A0+6j2f6gajnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"seq2seq.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "График средней ошибки, взятой на 100 последних батчах (ошибка на батче считается до обучения на нём, и каждый семпл участвует в обучение единожды)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd90441d290>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg0AAAFkCAYAAACjCwibAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBkmSVMj/B+JKh8zAi8l+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd90472cd50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(mean_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Для простоты и более быстрой сходимости генерирую предложения только длины 22-31, добавляю символ конца предложения - '.' и до 32-х добиваю символом padding - '?'. Генерирую батч из предложений равной длины (случайной в промежутке 22-31) и кормлю модели на одной итерации.\n",
    "\n",
    "В sequence loss было решено добавить ошибку на padding символах, потому что так модель обучалась лучше. Также настроил learning_rate. Начальные размер батча (64) и число нод (64) оказались оптимальными.\n",
    "\n",
    "При обучении вывожу результаты работы модели на предложениях из валидационной выборки (не участвующих в обучении). Видно как в начале модель учится определять конец предложения и выдавать паддинги после него. Вместо предложения сперва модель просто выдает пробелы, что дает гарантированно несколько матчингов (логично). После этого модель начинает распозновать позиции пробелов и выводить на места слов буквы вместо пробелов. По началу генерируются просто частотные символы, что опять же даёт гарантированый матчинг. Затем можно наблюдать, как алгоритм в действительности разворачивает некоторые слова полностью или частично.\n",
    "\n",
    "В конце концов у меня получилось научить модель довольно качественно разворачивать короткие слова (1-3 символа), особенно частотные типа 'the'. Также она неплохо справляется с длинными словами, правильно разворачивая хотя бы несколько символов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
